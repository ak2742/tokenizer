{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/tokenizer/blob/main/bpe_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to mount Google Drive at Colab Notebook instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3Fl0jHXASAkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Jump to the base tokenizer](#scrollTo=gO_LJ2f2kSBF)**\n",
        "[[Tokenizer](#scrollTo=gO_LJ2f2kSBF),\n",
        "[BasicTokenizer](#scrollTo=pg9sgBydoylB),\n",
        "[RegexTokenizer](#scrollTo=BVH-9nxEpkR_),\n",
        "[GPT4Tokenizer](#scrollTo=gnJnmtkVte1w)\n",
        "]"
      ],
      "metadata": {
        "id": "ALRY9oNg00Wg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pkHIk8j9BgP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title load data ``????``\n",
        "\n",
        "text = \"à¤¨à¤®à¤¸à¥à¤¤à¥‡ ðŸ’– World!!\"\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/data/shakespeare.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text += f.read()\n",
        "chars = sorted(set(text))\n",
        "\n",
        "# unicodes = [ord(x) for x in text]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get utf-8 codes for text ``!!!!``\n",
        "\n",
        "# tokens = text.encode(\"utf-8\")\n",
        "# tokens = list(map(int,tokens))\n",
        "# print(len(text))\n",
        "# print(len(tokens))"
      ],
      "metadata": {
        "id": "Y3LP-xJGSnXG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title fn to get the n of occurences of a pair in tokens ``!!!!``\n",
        "\n",
        "# def get_stats(ids):   #{pair -> n_occur}\n",
        "#     pairs = {}\n",
        "#     for i in range(len(ids) - 1):\n",
        "#         pair = tuple(ids[i:i+2])\n",
        "#         pairs[pair] = pairs.get(pair, 0) + 1\n",
        "#     return pairs\n"
      ],
      "metadata": {
        "id": "f5M1_JrqUe3_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title fn to merge a pair into a new token ``!!!!``\n",
        "\n",
        "# def merge_pairs(ids, pair, idx):\n",
        "#   # in a list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
        "#   new_ids = []\n",
        "#   i = 0\n",
        "#   while i < len(ids):\n",
        "#     if i < len(ids)-1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "#       new_ids.append(idx)\n",
        "#       i += 2\n",
        "#     else:\n",
        "#       new_ids.append(ids[i])\n",
        "#       i += 1\n",
        "#   return new_ids"
      ],
      "metadata": {
        "id": "Qn4f6W6VXnbL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title merge tokens upto vocab size ``!!!!``\n",
        "\n",
        "# vocab_size = 512  # max unique tokens after merge\n",
        "# num_merges = vocab_size-256\n",
        "# ids = list(tokens)\n",
        "\n",
        "# merges = {} # {pair -> new_token}\n",
        "\n",
        "# for i in range(num_merges):\n",
        "#   stats = get_stats(ids)\n",
        "#   top_pair = max(stats, key=stats.get)\n",
        "#   idx = 256 + i\n",
        "#   ids = merge_pairs(ids, top_pair, idx)\n",
        "#   merges[top_pair] = idx\n",
        "\n",
        "# merges"
      ],
      "metadata": {
        "id": "RIJyYW3Wga49",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title encoder-decoder ``!!!!``\n",
        "\n",
        "# vocab = {idx: bytes([idx]) for idx in range(256)} # {tokens -> bytes}\n",
        "# for (p0, p1), idx in merges.items():     # for merges\n",
        "#   vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "# def decode(ids):\n",
        "#   tokens = b\"\".join(vocab[i] for i in ids)\n",
        "#   text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "#   return text\n",
        "\n",
        "# def encode(text):\n",
        "#   tokens = list(text.encode(\"utf-8\"))\n",
        "#   while len(tokens) >= 2:\n",
        "#     stats = get_stats(tokens)\n",
        "#     pair = min(stats, key=lambda k: merges.get(k, float(\"inf\")))\n",
        "#     if pair not in merges:\n",
        "#       break\n",
        "#     idx = merges[pair]\n",
        "#     tokens = merge_pairs(tokens, pair, idx)\n",
        "#   return tokens\n"
      ],
      "metadata": {
        "id": "Fvy_nP49i7CJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title base class\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "def get_stats(ids, counts=None):\n",
        "    \"\"\"\n",
        "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
        "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
        "    Optionally allows to update an existing dictionary of counts\n",
        "    \"\"\"\n",
        "    counts = {} if counts is None else counts\n",
        "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    \"\"\"\n",
        "    In the list of integers (ids), replace all consecutive occurrences\n",
        "    of pair with the new integer token idx\n",
        "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
        "    \"\"\"\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        # if not at the very last position AND the pair matches, replace it\n",
        "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "# first two helper functions...\n",
        "def replace_control_characters(s: str) -> str:\n",
        "    # we don't want to print control characters\n",
        "    # which distort the output (e.g. \\n or much worse)\n",
        "    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python/19016117#19016117\n",
        "    # http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
        "    chars = []\n",
        "    for ch in s:\n",
        "        if unicodedata.category(ch)[0] != \"C\":\n",
        "            chars.append(ch) # this character is ok\n",
        "        else:\n",
        "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def render_token(t: bytes) -> str:\n",
        "    # pretty print a token, escaping control characters\n",
        "    s = t.decode('utf-8', errors='replace')\n",
        "    s = replace_control_characters(s)\n",
        "    return s\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Base class for Tokenizers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
        "        self.merges = {} # (int, int) -> int\n",
        "        self.pattern = \"\" # str\n",
        "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
        "        self.vocab = self._build_vocab() # int -> bytes\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Tokenizer can encode a string into a list of integers\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # Tokenizer can decode a list of integers into a string\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _build_vocab(self):\n",
        "        # vocab is simply and deterministically derived from merges\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        for special, idx in self.special_tokens.items():\n",
        "            vocab[idx] = special.encode(\"utf-8\")\n",
        "        return vocab\n",
        "\n",
        "    def save(self, file_prefix):\n",
        "        \"\"\"\n",
        "        Saves two files: file_prefix.vocab and file_prefix.model\n",
        "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
        "        - model file is the critical one, intended for load()\n",
        "        - vocab file is just a pretty printed version for human inspection only\n",
        "        \"\"\"\n",
        "        # write the model: to be used in load() later\n",
        "        model_file = file_prefix + \".model\"\n",
        "        with open(model_file, 'w') as f:\n",
        "            # write the version, pattern and merges, that's all that's needed\n",
        "            f.write(\"rsbpe v1\\n\")\n",
        "            f.write(f\"{self.pattern}\\n\")\n",
        "            # write the special tokens, first the number of them, then each one\n",
        "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
        "            for special, idx in self.special_tokens.items():\n",
        "                f.write(f\"{special} {idx}\\n\")\n",
        "            # the merges dict\n",
        "            for idx1, idx2 in self.merges:\n",
        "                f.write(f\"{idx1} {idx2}\\n\")\n",
        "        # write the vocab: for the human to look at\n",
        "        vocab_file = file_prefix + \".vocab\"\n",
        "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for idx, token in self.vocab.items():\n",
        "                # note: many tokens may be partial utf-8 sequences\n",
        "                # and cannot be decoded into valid strings. Here we're using\n",
        "                # errors='replace' to replace them with the replacement char ï¿½.\n",
        "                # this also means that we couldn't possibly use .vocab in load()\n",
        "                # because decoding in this way is a lossy operation!\n",
        "                s = render_token(token)\n",
        "                # find the children of this token, if any\n",
        "                if idx in inverted_merges:\n",
        "                    # if this token has children, render it nicely as a merge\n",
        "                    idx0, idx1 = inverted_merges[idx]\n",
        "                    s0 = render_token(self.vocab[idx0])\n",
        "                    s1 = render_token(self.vocab[idx1])\n",
        "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
        "                else:\n",
        "                    # otherwise this is leaf token, just print it\n",
        "                    # (this should just be the first 256 tokens, the bytes)\n",
        "                    f.write(f\"[{s}] {idx}\\n\")\n",
        "\n",
        "    def load(self, model_file):\n",
        "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
        "        assert model_file.endswith(\".model\")\n",
        "        # read the model file\n",
        "        merges = {}\n",
        "        special_tokens = {}\n",
        "        idx = 256\n",
        "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
        "            # read the version\n",
        "            version = f.readline().strip()\n",
        "            assert version == \"rsbpe v1\"\n",
        "            # read the pattern\n",
        "            self.pattern = f.readline().strip()\n",
        "            # read the special tokens\n",
        "            num_special = int(f.readline().strip())\n",
        "            for _ in range(num_special):\n",
        "                special, special_idx = f.readline().strip().split()\n",
        "                special_tokens[special] = int(special_idx)\n",
        "            # read the merges\n",
        "            for line in f:\n",
        "                idx1, idx2 = map(int, line.split())\n",
        "                merges[(idx1, idx2)] = idx\n",
        "                idx += 1\n",
        "        self.merges = merges\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab = self._build_vocab()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gO_LJ2f2kSBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title a basic tokenizer\n",
        "\n",
        "class BasicTokenizer(Tokenizer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        assert vocab_size >= 256\n",
        "        num_merges = vocab_size - 256\n",
        "\n",
        "        # input text preprocessing\n",
        "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
        "        ids = list(text_bytes) # list of integers in range 0..255\n",
        "\n",
        "        # iteratively merge the most common pairs to create new tokens\n",
        "        merges = {} # (int, int) -> int\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
        "        for i in range(num_merges):\n",
        "            # count up the number of times every consecutive pair appears\n",
        "            stats = get_stats(ids)\n",
        "            # find the pair with the highest count\n",
        "            pair = max(stats, key=stats.get)\n",
        "            # mint a new token: assign it the next available id\n",
        "            idx = 256 + i\n",
        "            # replace all occurrences of pair in ids with idx\n",
        "            ids = merge(ids, pair, idx)\n",
        "            # save the merge\n",
        "            merges[pair] = idx\n",
        "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "            # prints\n",
        "            if verbose:\n",
        "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
        "\n",
        "        # save class variables\n",
        "        self.merges = merges # used in encode()\n",
        "        self.vocab = vocab   # used in decode()\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # given ids (list of integers), return Python string\n",
        "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
        "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text\n",
        "\n",
        "    def encode(self, text):\n",
        "        # given a string text, return the token ids\n",
        "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
        "        ids = list(text_bytes) # list of integers in range 0..255\n",
        "        while len(ids) >= 2:\n",
        "            # find the pair with the lowest merge index\n",
        "            stats = get_stats(ids)\n",
        "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "            # subtle: if there are no more merges available, the key will\n",
        "            # result in an inf for every single pair, and the min will be\n",
        "            # just the first pair in the list, arbitrarily\n",
        "            # we can detect this terminating case by a membership check\n",
        "            if pair not in self.merges:\n",
        "                break # nothing else can be merged anymore\n",
        "            # otherwise let's merge the best pair (lowest merge index)\n",
        "            idx = self.merges[pair]\n",
        "            ids = merge(ids, pair, idx)\n",
        "        return ids\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pg9sgBydoylB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title using regex\n",
        "\n",
        "import regex as re\n",
        "\n",
        "\n",
        "# the main GPT text split patterns, see\n",
        "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
        "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "\n",
        "class RegexTokenizer(Tokenizer):\n",
        "\n",
        "    def __init__(self, pattern=None):\n",
        "        \"\"\"\n",
        "        - pattern: optional string to override the default (GPT-4 split pattern)\n",
        "        - special_tokens: str -> int dictionary of special tokens\n",
        "          example: {'<|endoftext|>': 100257}\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
        "        self.compiled_pattern = re.compile(self.pattern)\n",
        "        self.special_tokens = {}\n",
        "        self.inverse_special_tokens = {}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        assert vocab_size >= 256\n",
        "        num_merges = vocab_size - 256\n",
        "\n",
        "        # split the text up into text chunks\n",
        "        text_chunks = re.findall(self.compiled_pattern, text) # [\"\", \"\", \"\"]\n",
        "\n",
        "        # input text preprocessing\n",
        "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks] # [[1, 2, 3], [2, 4, 5]]\n",
        "\n",
        "        # iteratively merge the most common pairs to create new tokens\n",
        "        merges = {} # (int, int) -> int\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
        "        for i in range(num_merges):\n",
        "            # count the number of times every consecutive pair appears\n",
        "            stats = {}\n",
        "            for chunk_ids in ids:\n",
        "                # passing in stats will update it in place, adding up counts\n",
        "                get_stats(chunk_ids, stats)\n",
        "            # find the pair with the highest count\n",
        "            pair = max(stats, key=stats.get)\n",
        "            # mint a new token: assign it the next available id\n",
        "            idx = 256 + i\n",
        "            # replace all occurrences of pair in ids with idx\n",
        "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
        "            # save the merge\n",
        "            merges[pair] = idx\n",
        "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "            # prints\n",
        "            if verbose:\n",
        "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
        "\n",
        "        # save class variables\n",
        "        self.merges = merges # used in encode()\n",
        "        self.vocab = vocab   # used in decode()\n",
        "\n",
        "    def register_special_tokens(self, special_tokens):\n",
        "        # special_tokens is a dictionary of str -> int\n",
        "        # example: {\"<|endoftext|>\": 100257}\n",
        "        self.special_tokens = special_tokens\n",
        "        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # given ids (list of integers), return Python string\n",
        "        part_bytes = []\n",
        "        for idx in ids:\n",
        "            if idx in self.vocab:\n",
        "                part_bytes.append(self.vocab[idx])\n",
        "            elif idx in self.inverse_special_tokens:\n",
        "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
        "            else:\n",
        "                raise ValueError(f\"invalid token id: {idx}\")\n",
        "        text_bytes = b\"\".join(part_bytes)\n",
        "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text\n",
        "\n",
        "    def _encode_chunk(self, text_bytes):\n",
        "        # return the token ids\n",
        "        # let's begin. first, convert all bytes to integers in range 0..255\n",
        "        ids = list(text_bytes)\n",
        "        while len(ids) >= 2:\n",
        "            # find the pair with the lowest merge index\n",
        "            stats = get_stats(ids)\n",
        "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "            # subtle: if there are no more merges available, the key will\n",
        "            # result in an inf for every single pair, and the min will be\n",
        "            # just the first pair in the list, arbitrarily\n",
        "            # we can detect this terminating case by a membership check\n",
        "            if pair not in self.merges:\n",
        "                break # nothing else can be merged anymore\n",
        "            # otherwise let's merge the best pair (lowest merge index)\n",
        "            idx = self.merges[pair]\n",
        "            ids = merge(ids, pair, idx)\n",
        "        return ids\n",
        "\n",
        "    def encode_ordinary(self, text):\n",
        "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
        "        # split text into chunks of text by categories defined in regex pattern\n",
        "        text_chunks = re.findall(self.compiled_pattern, text)\n",
        "        # all chunks of text are encoded separately, then results are joined\n",
        "        ids = []\n",
        "        for chunk in text_chunks:\n",
        "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
        "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
        "            ids.extend(chunk_ids)\n",
        "        return ids\n",
        "\n",
        "    def encode(self, text, allowed_special=\"none_raise\"):\n",
        "        \"\"\"\n",
        "        Unlike encode_ordinary, this function handles special tokens.\n",
        "        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
        "        if none_raise, then an error is raised if any special token is encountered in text\n",
        "        this is the default tiktoken behavior right now as well\n",
        "        any other behavior is either annoying, or a major footgun\n",
        "        \"\"\"\n",
        "        # decode the user desire w.r.t. handling of special tokens\n",
        "        special = None\n",
        "        if allowed_special == \"all\":\n",
        "            special = self.special_tokens\n",
        "        elif allowed_special == \"none\":\n",
        "            special = {}\n",
        "        elif allowed_special == \"none_raise\":\n",
        "            special = {}\n",
        "            assert all(token not in text for token in self.special_tokens)\n",
        "        elif isinstance(allowed_special, set):\n",
        "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
        "        else:\n",
        "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
        "        if not special:\n",
        "            # shortcut: if no special tokens, just use the ordinary encoding\n",
        "            return self.encode_ordinary(text)\n",
        "        # otherwise, we have to be careful with potential special tokens in text\n",
        "        # we handle special tokens by splitting the text\n",
        "        # based on the occurrence of any exact match with any of the special tokens\n",
        "        # we can use re.split for this. note that surrounding the pattern with ()\n",
        "        # makes it into a capturing group, so the special tokens will be included\n",
        "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
        "        special_chunks = re.split(special_pattern, text)\n",
        "        # now all the special characters are separated from the rest of the text\n",
        "        # all chunks of text are encoded separately, then results are joined\n",
        "        ids = []\n",
        "        for part in special_chunks:\n",
        "            if part in special:\n",
        "                # this is a special token, encode it separately as a special case\n",
        "                ids.append(special[part])\n",
        "            else:\n",
        "                # this is an ordinary sequence, encode it normally\n",
        "                ids.extend(self.encode_ordinary(part))\n",
        "        return ids\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BVH-9nxEpkR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tiktoken"
      ],
      "metadata": {
        "id": "wSrujaVIz_SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title gpt4 tokenizer\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "def bpe(mergeable_ranks, token, max_rank):\n",
        "    # helper function used in get_gpt4_merges() to reconstruct the merge forest\n",
        "    parts = [bytes([b]) for b in token]  # Uni\n",
        "    while True:\n",
        "        min_idx = None\n",
        "        min_rank = None\n",
        "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
        "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
        "            if rank is not None and (min_rank is None or rank < min_rank):\n",
        "                min_idx = i\n",
        "                min_rank = rank\n",
        "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
        "            break\n",
        "        assert min_idx is not None\n",
        "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
        "    return parts\n",
        "\n",
        "\n",
        "def recover_merges(mergeable_ranks):\n",
        "    # the `merges` are already the byte sequences in their merged state.\n",
        "    # so we have to recover the original pairings. We can do this by doing\n",
        "    # a small BPE training run on all the tokens, in their order.\n",
        "    # also see https://github.com/openai/tiktoken/issues/60\n",
        "    # also see https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306\n",
        "    merges = {}\n",
        "    for token, rank in mergeable_ranks.items():\n",
        "        if len(token) == 1:\n",
        "            continue # skip raw bytes\n",
        "        pair = tuple(bpe(mergeable_ranks, token, max_rank=rank)) # (,)\n",
        "        assert len(pair) == 2\n",
        "        # recover the integer ranks of the pair\n",
        "        ix0 = mergeable_ranks[pair[0]]\n",
        "        ix1 = mergeable_ranks[pair[1]]\n",
        "        merges[(ix0, ix1)] = rank\n",
        "\n",
        "    return merges\n",
        "\n",
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "GPT4_SPECIAL_TOKENS = {\n",
        "    '<|endoftext|>': 100257,\n",
        "    '<|fim_prefix|>': 100258,\n",
        "    '<|fim_middle|>': 100259,\n",
        "    '<|fim_suffix|>': 100260,\n",
        "    '<|endofprompt|>': 100276\n",
        "}\n",
        "\n",
        "class GPT4Tokenizer(RegexTokenizer):\n",
        "    \"\"\"Lightweight wrapper on RegexTokenizer that matches GPT-4's tokenizer.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(pattern=GPT4_SPLIT_PATTERN)\n",
        "        # get the official tokenizer and its merges\n",
        "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        mergeable_ranks = enc._mergeable_ranks\n",
        "        # print(len(mergeable_ranks))\n",
        "        # the merges are those of gpt4, but we have to recover them\n",
        "        self.merges = recover_merges(mergeable_ranks)\n",
        "        # print(len(self.merges))\n",
        "        # reconstruct the vocab from the merges\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        self.vocab = vocab\n",
        "        # now here is another tricky part.\n",
        "        # for some reason, the tokens corresponding to individual bytes\n",
        "        # are permuted in a different order. This is completely non-sensical\n",
        "        # and probably historical, but therefore we have to deal with it here.\n",
        "        self.byte_shuffle = {i: mergeable_ranks[bytes([i])] for i in range(256)} # Uni -> tik\n",
        "        # print(self.byte_shuffle)\n",
        "        self.inverse_byte_shuffle = {v: k for k, v in self.byte_shuffle.items()}\n",
        "        # finally register the special tokens\n",
        "        self.register_special_tokens(GPT4_SPECIAL_TOKENS)\n",
        "\n",
        "    def _encode_chunk(self, text_bytes):\n",
        "        # before we start processing bytes, we have to permute them\n",
        "        text_bytes = bytes(self.byte_shuffle[b] for b in text_bytes)\n",
        "        ids = super()._encode_chunk(text_bytes)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # we have to un-permute the bytes before we decode\n",
        "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
        "        text_bytes = bytes(self.inverse_byte_shuffle[b] for b in text_bytes)\n",
        "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text\n",
        "\n",
        "    # this is a pretrained tokenizer, it is not intended to be trained\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # save/load would require some thought.\n",
        "    # we'd have to change save/load of base to add support for byte_shuffle...\n",
        "    # alternatively, we could move byte_shuffle to base class, but that would\n",
        "    # mean that we're making ugly our beautiful Tokenizer just to support\n",
        "    # the GPT-4 tokenizer and its weird historical quirks around byte_shuffle.\n",
        "    def save(self, file_prefix):\n",
        "        raise NotImplementedError(\"GPT4Tokenizer cannot be saved.\")\n",
        "\n",
        "    def load(self, model_file):\n",
        "        raise NotImplementedError(\"GPT4Tokenizer cannot be loaded.\")\n",
        "\n",
        "    def save_vocab(self, vocab_file):\n",
        "        # just for visualization purposes let's output the GPT-4 tokens\n",
        "        # in the exact same format as the base class would.\n",
        "        # simple run as:\n",
        "        # python -c \"from minbpe import GPT4Tokenizer; GPT4Tokenizer().save_vocab('gpt4.vocab')\"\n",
        "\n",
        "        # build vocab being mindful of the byte shuffle\n",
        "        vocab = {idx: bytes([self.inverse_byte_shuffle[idx]]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        # now merge the shuffled bytes and write to file\n",
        "        # print(vocab)\n",
        "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for idx, token in vocab.items():\n",
        "                s = render_token(token)\n",
        "                if idx in inverted_merges:\n",
        "                    idx0, idx1 = inverted_merges[idx]\n",
        "                    s0 = render_token(vocab[idx0])\n",
        "                    s1 = render_token(vocab[idx1])\n",
        "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
        "                else:\n",
        "                    f.write(f\"[{s}] {idx}\\n\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gnJnmtkVte1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}